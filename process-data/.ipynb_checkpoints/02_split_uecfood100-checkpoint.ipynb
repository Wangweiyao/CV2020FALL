{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_split_uecfood100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split UECFOOD256 dataset to training, val, testing sets with ratio 0.7, 0.2, 0.1\n",
    "- Save img_dir, category_id, x1, y1, x2, y2 into txt file under train_uec256.txt, val_uec256.txt and test_uec256.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_disk = '/home/weiyao/data/'\n",
    "#     uecfood100_path = dataset_disk + 'UECFOOD100_448'\n",
    "uecfood100_path = dataset_disk + 'UECFOOD100'\n",
    "wwfood100_path = dataset_disk + 'food53'\n",
    "category = 'category.txt'\n",
    "bbox_info = 'new_bb_info.txt'\n",
    "\n",
    "# Put first column (id) and second column (name) from category.txt into two lists\n",
    "category_ids = []\n",
    "category_names = []\n",
    "with open(uecfood100_path + '/' + category, 'r') as category_list:\n",
    "    for i, line in enumerate(category_list):\n",
    "        if i > 0:\n",
    "            line = line.rstrip('\\n')  # delete \\n in the end of th\n",
    "            # e line\n",
    "            line = line.split('\\t')\n",
    "            category_ids.append(int(line[0]))\n",
    "            category_names.append(line[1])\n",
    "\n",
    "# Read bb_info.txt based on category id\n",
    "category_images = []\n",
    "category_bbox = []\n",
    "for id_index, id in enumerate(category_ids):\n",
    "    category_images.append([])\n",
    "    category_bbox.append([])\n",
    "    with open(uecfood100_path + '/' + str(id) + '/' + bbox_info, 'r') as bbox_list:\n",
    "        for i, line in enumerate(bbox_list):\n",
    "            if i > 0:\n",
    "                line = line.rstrip('\\n')\n",
    "                line = line.split(' ')\n",
    "                if int(line[0]) in [14577, 15367, 13645, 2486, 12064, 15216, 13882, 15469]:\n",
    "                    continue # dont include data with wrong image shape\n",
    "                category_images[id_index].append(line[0])\n",
    "                category_bbox[id_index].append(list(map(float, line[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_256_images = glob.glob('/home/weiyao/data/yolov5_food256_cleaned/images/train/*') + glob.glob('/home/weiyao/data/yolov5_food256_cleaned/images/val/*')\n",
    "clean_256_images_idxs = []\n",
    "for i in range(len(clean_256_images)):\n",
    "    clean_256_images_idxs.append(int(clean_256_images[i].split('/')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27954\n"
     ]
    }
   ],
   "source": [
    "all_clean_list = []\n",
    "with open('all_cleaned.txt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.rstrip('\\n')\n",
    "        all_clean_list.append(int(line))\n",
    "print(len(all_clean_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left 13530 images\n"
     ]
    }
   ],
   "source": [
    "new_category_images = []\n",
    "count = 0\n",
    "for img_list in category_images:\n",
    "    new_list = []\n",
    "    for img in img_list:\n",
    "        if int(img) in clean_256_images_idxs:\n",
    "            new_list.append(img)\n",
    "            count+=1\n",
    "    new_category_images.append(new_list)\n",
    "\n",
    "category_images = new_category_images\n",
    "print(f'Left {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left 13524 images\n"
     ]
    }
   ],
   "source": [
    "new_category_images = []\n",
    "count = 0\n",
    "for img_list in category_images:\n",
    "    new_list = []\n",
    "    for img in img_list:\n",
    "        if int(img) in all_clean_list:\n",
    "            new_list.append(img)\n",
    "            count+=1\n",
    "    new_category_images.append(new_list)\n",
    "\n",
    "category_images = new_category_images\n",
    "print(f'Left {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "def plot_class_ex(class_id):\n",
    "    print(category_names[class_id-1])\n",
    "    plt.figure()\n",
    "    img_path = glob.glob(uecfood100_path+f'/{class_id}/*.jpg')[1]\n",
    "    img = cv2.imread(img_path)\n",
    "    plt.imshow(img[:,:,::-1])\n",
    "    plt.figure()\n",
    "    img_path = glob.glob(uecfood100_path+f'/{class_id}/*.jpg')[-2]\n",
    "    img = cv2.imread(img_path)\n",
    "    plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_map_dict():\n",
    "    new_class_dict = {}\n",
    "    new_class_dict['rice'] = ['rice',]\n",
    "    new_class_dict['rice bowl'] = ['eels on rice',\"chicken-'n'-egg on rice\", \"pork cutlet on rice\", \"tempura bowl\",\"bibimbap\",\"beef bowl\"]\n",
    "    new_class_dict['fried rice'] = ['pilaf','chicken rice','fried rice',\"mixed rice\"]\n",
    "    new_class_dict['curry bowl'] = ['beef curry',\"cutlet curry\"]\n",
    "    new_class_dict['sushi'] = [\"sushi\"]\n",
    "    new_class_dict['toast'] = [\"toast\",\"pizza toast\"]\n",
    "    new_class_dict['croissant'] = [\"croissant\"]\n",
    "    new_class_dict['bread'] = [\"roll bread\",\"raisin bread\",]\n",
    "    new_class_dict['hamburger'] = [\"hamburger\",]\n",
    "    new_class_dict['pizza'] = [\"pizza\",]\n",
    "    new_class_dict['sandwiches'] = [\"sandwiches\",]\n",
    "    new_class_dict['noodle or ramen'] = [\"udon noodle\",\"tempura udon\",\"soba noodle\",\"ramen noodle\",\"beef noodle\",\"tensin noodle\",\"dipping noodles\"]\n",
    "    new_class_dict['fried noodle'] = [\"fried noodle\"]\n",
    "    new_class_dict['spaghetti'] = [\"spaghetti\",\"spaghetti meat sauce\"]\n",
    "    new_class_dict['stir fry or boiled vegetables'] = [\"sauteed vegetables\",\"grilled eggplant\",\"sauteed spinach\",\"kinpira-style sauteed burdock\",\"goya chanpuru\"]\n",
    "    new_class_dict['omelet'] = [\"omelet\",\"rolled omelet\",\"omelet with fried rice\"]\n",
    "    new_class_dict['dumpling'] = [\"jiaozi\",\"steamed meat dumpling\"]\n",
    "    new_class_dict['stew'] = [\"stew\",\"seasoned beef with potatoes\"]\n",
    "    new_class_dict['fish'] = [\"teriyaki grilled fish\",\"grilled salmon\",\"salmon meuniere\",\"grilled pacific saury\",\"lightly roasted fish\",\"nanbanzuke\",\"boiled fish\",\"dried fish\",\"fried fish\",]\n",
    "    new_class_dict['sashimi'] = [\"sashimi\",]\n",
    "    new_class_dict['hot pot'] = [\"sukiyaki\",]\n",
    "    new_class_dict['stir fried or boil meat'] = [\"sweet and sour pork\",\"ginger pork saute\",\"stir-fried beef and peppers\",\"boiled chicken and vegetables\",]\n",
    "    new_class_dict['fried chicken'] = [\"fried chicken\",]\n",
    "    new_class_dict['steak'] = [\"hambarg steak\",\"beef steak\"]\n",
    "    new_class_dict['egg'] = [\"egg sunny-side up\",]\n",
    "    new_class_dict['shrimp'] = [\"shrimp with chill source\",\"fried shrimp\"]\n",
    "    new_class_dict['roast chicken'] = [\"roast chicken\",]\n",
    "    new_class_dict['salad'] = [\"potato salad\",\"green salad\",\"macaroni salad\"]\n",
    "    new_class_dict['soup'] = [\"Japanese tofu and vegetable chowder\",\"pork miso soup\",\"chinese soup\",\"miso soup\"]\n",
    "    new_class_dict['hot dog'] = [\"hot dog\"]\n",
    "    new_class_dict['french fries'] = [\"french fries\"]\n",
    "    new_class_dict['tofu'] = [\"spicy chili-flavored tofu\",\"cold tofu\"]\n",
    "\n",
    "    new_class_dict['chip butty'] = [\"chip butty\",]\n",
    "    new_class_dict['Japanese-style pancake'] = [\"Japanese-style pancake\",]\n",
    "    new_class_dict['takoyaki'] = [\"takoyaki\",]\n",
    "    new_class_dict['gratin'] = [\"gratin\",]\n",
    "    new_class_dict['croquette'] = [\"croquette\",]\n",
    "    new_class_dict['tempura'] = [\"tempura\",\"vegetable tempura\",]\n",
    "    new_class_dict['potage'] = [\"potage\",]\n",
    "    new_class_dict['sausage'] = [\"sausage\",]\n",
    "    new_class_dict['oden'] = [\"oden\",]\n",
    "    new_class_dict['ganmodoki'] = [\"ganmodoki\",]\n",
    "    new_class_dict['steamed egg hotchpotch'] = [\"steamed egg hotchpotch\",]\n",
    "    new_class_dict['sirloin cutlet'] = [\"sirloin cutlet\",]\n",
    "    new_class_dict['skewer'] = [\"yakitori\",]\n",
    "    new_class_dict['cabbage roll'] = [\"cabbage roll\",]\n",
    "    new_class_dict['fermented soybeans'] = [\"fermented soybeans\",]\n",
    "    new_class_dict['egg roll'] = [\"egg roll\",]\n",
    "    new_class_dict['chilled noodle'] = [\"chilled noodle\",]\n",
    "    new_class_dict['simmered meat'] = [\"simmered pork\",]\n",
    "    new_class_dict['fish bowl'] = [\"sushi bowl\",\"sashimi bowl\",]\n",
    "    new_class_dict['fish-shaped pancake with bean jam'] = [\"fish-shaped pancake with bean jam\",]\n",
    "    new_class_dict['rice ball'] = [\"rice ball\",]\n",
    "    \n",
    "    map_dict = {}\n",
    "    for new_class in new_class_dict.keys():\n",
    "        old_classes = new_class_dict[new_class]\n",
    "        for old_class in old_classes:\n",
    "            map_dict[old_class] = new_class\n",
    "    \n",
    "    return map_dict, new_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict, new_class_dict = get_map_dict()\n",
    "new_category_names = list(new_class_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_category_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_id, category_name in enumerate(category_names):\n",
    "    new_category_name = map_dict[category_name]\n",
    "    if new_category_name not in new_category_count.keys():\n",
    "        new_category_count[new_category_name] = len(category_images[category_id])\n",
    "    else:\n",
    "        new_category_count[new_category_name] += len(category_images[category_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30., 11.,  3.,  1.,  3.,  1.,  1.,  0.,  0.,  3.]),\n",
       " array([  93. ,  189.7,  286.4,  383.1,  479.8,  576.5,  673.2,  769.9,\n",
       "         866.6,  963.3, 1060. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([v for k, v in sorted(new_category_count.items(), key=lambda item: item[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['rice', 'rice bowl', 'fried rice', 'curry bowl', 'toast', 'bread', 'hamburger', 'noodle or ramen', 'spaghetti', 'stir fry or boiled vegetables', 'tempura', 'soup', 'omelet', 'dumpling', 'stew', 'fish', 'stir fried or boil meat', 'steak', 'tofu', 'egg', 'fish bowl', 'shrimp', 'salad']\n",
      "30\n",
      "['sushi', 'croissant', 'chip butty', 'pizza', 'sandwiches', 'fried noodle', 'Japanese-style pancake', 'takoyaki', 'gratin', 'croquette', 'potage', 'sausage', 'oden', 'ganmodoki', 'sashimi', 'hot pot', 'steamed egg hotchpotch', 'fried chicken', 'sirloin cutlet', 'skewer', 'cabbage roll', 'fermented soybeans', 'egg roll', 'chilled noodle', 'simmered meat', 'fish-shaped pancake with bean jam', 'roast chicken', 'rice ball', 'hot dog', 'french fries']\n"
     ]
    }
   ],
   "source": [
    "base_labels = [key for (key,value) in new_category_count.items() if value >=200]\n",
    "print(len(base_labels))\n",
    "print(base_labels)\n",
    "novel_labels = [key for (key,value) in new_category_count.items() if value <200]\n",
    "print(len(novel_labels))\n",
    "print(novel_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(wwfood100_path + '/labels.txt', 'w')\n",
    "for c in category_names:\n",
    "    file.write(c + '\\n')\n",
    "file.close()\n",
    "\n",
    "file = open(wwfood100_path + '/new_labels.txt', 'w')\n",
    "for c in new_category_names:\n",
    "    file.write(c + '\\n')\n",
    "file.close()\n",
    "\n",
    "file = open(wwfood100_path + '/base_labels.txt', 'w')\n",
    "for c in base_labels:\n",
    "    file.write(c + '\\n')\n",
    "file.close()\n",
    "\n",
    "file = open(wwfood100_path + '/novel_labels.txt', 'w')\n",
    "for c in novel_labels:\n",
    "    file.write(c + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [0.7, 0.2, 0.1]\n",
    "files_generated = ['all_train.txt', 'all_val.txt', 'all_test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split categories to train/val/test with ratio define before\n",
    "train_uec100 = []\n",
    "val_uec100 = []\n",
    "test_uec100 = []\n",
    "for id_index, id in enumerate(category_ids):\n",
    "    # divide each category with 70% training, 20% val, 10% testing\n",
    "    n_imgs = len(category_images[id_index])\n",
    "    n_train = int(np.floor(n_imgs * split[0]))\n",
    "    n_val = int(np.floor(n_imgs * split[1]))\n",
    "    n_test = int(n_imgs - n_train - n_val)\n",
    "\n",
    "    # shuffle images\n",
    "    shuffled_imgs = random.Random(0).sample(category_images[id_index], n_imgs)\n",
    "\n",
    "    train_uec100.append(shuffled_imgs[:n_train])  # not including the last one\n",
    "    val_uec100.append(shuffled_imgs[n_train:n_train + n_val])\n",
    "    test_uec100.append(shuffled_imgs[n_train + n_val:])\n",
    "\n",
    "all_train_list = list(np.unique(list(itertools.chain(*train_uec100))))\n",
    "all_val_list = list(np.unique(list(itertools.chain(*val_uec100))))\n",
    "all_test_list = list(np.unique(list(itertools.chain(*test_uec100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8465\n",
      "2578\n",
      "1418\n"
     ]
    }
   ],
   "source": [
    "print(len(all_train_list))\n",
    "print(len(all_val_list))\n",
    "print(len(all_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop out element in training set if it's in testing or val also\n",
    "new_all_train_list = []\n",
    "new_all_val_list = []\n",
    "new_all_test_list = []\n",
    "\n",
    "for img_id in all_train_list:\n",
    "    if img_id in all_val_list or img_id in all_test_list:\n",
    "        if np.random.rand() < split[0]:\n",
    "            new_all_train_list.append(img_id)\n",
    "    else:\n",
    "        new_all_train_list.append(img_id)\n",
    "            \n",
    "    \n",
    "for img_id in all_val_list:\n",
    "    if not(img_id in new_all_train_list):\n",
    "        if img_id in all_test_list:\n",
    "            if np.random.rand() < (split[1]/(split[1]+split[2])):\n",
    "                new_all_val_list.append(img_id)\n",
    "        else:\n",
    "            new_all_val_list.append(img_id)\n",
    "            \n",
    "for img_id in all_test_list:\n",
    "    if not(img_id in new_all_train_list) and not(img_id in new_all_val_list):\n",
    "        new_all_test_list.append(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(new_all_test_list) &w set(new_all_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_all_train_list))\n",
    "print(len(new_all_val_list))\n",
    "print(len(new_all_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_list = new_all_train_list\n",
    "all_val_list = new_all_val_list\n",
    "all_test_list = new_all_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split bounding box with train, val, test sets\n",
    "imgs_format = 'jpg'\n",
    "file = open(wwfood100_path + '/labels.txt', 'w')\n",
    "for c in category_names:\n",
    "    file.write(c + '\\n')\n",
    "file.close()\n",
    "\n",
    "#### Training set\n",
    "file = open(wwfood100_path + '/' + files_generated[0], 'w')\n",
    "#file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "for img in all_train_list:\n",
    "    # it is possible that one image in several categories\n",
    "    occurrences = []\n",
    "    for id_index, id in enumerate(category_ids):\n",
    "        occ = [[wwfood100_path + '/Images/' + img + '.' + imgs_format, str(id)] +\n",
    "               category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "        occurrences += occ\n",
    "\n",
    "    for occ in occurrences:\n",
    "        img_path = occ[0]\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        file.write(img_id + '\\n')\n",
    "        #img_category = occ[1]\n",
    "        #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "        #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "file.close()\n",
    "\n",
    "#### Val set\n",
    "file = open(wwfood100_path + '/' + files_generated[1], 'w')\n",
    "#file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "for img in all_val_list:\n",
    "    # it is possible that one image in several categories\n",
    "    occurrences = []\n",
    "    for id_index, id in enumerate(category_ids):\n",
    "        occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] +\n",
    "               category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "        occurrences += occ\n",
    "\n",
    "    for occ in occurrences:\n",
    "        img_path = occ[0]\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        file.write(img_id + '\\n')\n",
    "        #img_category = occ[1]\n",
    "        #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "        #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "file.close()\n",
    "\n",
    "#### Testing set\n",
    "file = open(wwfood100_path + '/' + files_generated[2], 'w')\n",
    "#file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "for img in all_test_list:\n",
    "    # it is possible that one image in several categories\n",
    "    occurrences = []\n",
    "    for id_index, id in enumerate(category_ids):\n",
    "        occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] +\n",
    "               category_bbox[id_index][i] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "        occurrences += occ\n",
    "\n",
    "    for occ in occurrences:\n",
    "        img_path = occ[0]\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        file.write(img_id + '\\n')\n",
    "        #img_category = occ[1]\n",
    "        #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "        #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "file.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base/novel split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_original_category_idxs = []\n",
    "for idx,category_name in enumerate(category_names):\n",
    "    new_category_name = map_dict[category_name]\n",
    "    if new_category_name in base_labels:\n",
    "        base_original_category_idxs.append(idx)\n",
    "        \n",
    "novel_original_category_idxs = []\n",
    "for idx,category_name in enumerate(category_names):\n",
    "    new_category_name = map_dict[category_name]\n",
    "    if new_category_name in novel_labels:\n",
    "        novel_original_category_idxs.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_category_images = [category_images[idx] for idx in base_original_category_idxs]\n",
    "novel_category_images = [category_images[idx] for idx in novel_original_category_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_category_images_raw = [item for sublist in base_category_images for item in sublist]\n",
    "novel_category_images_raw = [item for sublist in novel_category_images for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = set(base_category_images_raw) & set(novel_category_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(base_category_images_raw))\n",
    "print(len(novel_category_images_raw))\n",
    "print(len(intersection_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screened_base_category_images = []\n",
    "for images in base_category_images:\n",
    "    screened_base_category_images.append([img for img in images if img not in intersection_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screened_base_category_images_raw = [item for sublist in screened_base_category_images for item in sublist]\n",
    "len(screened_base_category_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def save_txt(files_generated, split, category_ids, category_images):\n",
    "    # Split categories to train/val/test with ratio define before\n",
    "    train_uec100 = []\n",
    "    val_uec100 = []\n",
    "    test_uec100 = []\n",
    "    for id_index, id in enumerate(category_ids):\n",
    "        # divide each category with 70% training, 20% val, 10% testing\n",
    "        n_imgs = len(category_images[id_index])\n",
    "        n_train = int(np.floor(n_imgs * split[0]))\n",
    "        n_val = int(np.floor(n_imgs * split[1]))\n",
    "        n_test = int(n_imgs - n_train - n_val)\n",
    "\n",
    "        # shuffle images\n",
    "        shuffled_imgs = random.Random(0).sample(category_images[id_index], n_imgs)\n",
    "\n",
    "        train_uec100.append(shuffled_imgs[:n_train])  # not including the last one\n",
    "        val_uec100.append(shuffled_imgs[n_train:n_train + n_val])\n",
    "        test_uec100.append(shuffled_imgs[n_train + n_val:])\n",
    "\n",
    "    all_train_list = list(np.unique(list(itertools.chain(*train_uec100))))\n",
    "    all_val_list = list(np.unique(list(itertools.chain(*val_uec100))))\n",
    "    all_test_list = list(np.unique(list(itertools.chain(*test_uec100))))\n",
    "\n",
    "\n",
    "    # Pop out element in training set if it's in testing or val also\n",
    "    new_all_train_list = []\n",
    "    new_all_val_list = []\n",
    "    new_all_test_list = []\n",
    "\n",
    "    for img_id in all_train_list:\n",
    "        if img_id in all_val_list or img_id in all_test_list:\n",
    "            if np.random.rand() < split[0]:\n",
    "                new_all_train_list.append(img_id)\n",
    "        else:\n",
    "            new_all_train_list.append(img_id)\n",
    "\n",
    "\n",
    "    for img_id in all_val_list:\n",
    "        if not(img_id in new_all_train_list):\n",
    "            if img_id in all_test_list:\n",
    "                if np.random.rand() < (split[1]/(split[1]+split[2])):\n",
    "                    new_all_val_list.append(img_id)\n",
    "            else:\n",
    "                new_all_val_list.append(img_id)\n",
    "\n",
    "    for img_id in all_test_list:\n",
    "        if not(img_id in new_all_train_list) and not(img_id in new_all_val_list):\n",
    "            new_all_test_list.append(img_id)\n",
    "        \n",
    "\n",
    "    all_train_list = new_all_train_list\n",
    "    all_val_list = new_all_val_list\n",
    "    all_test_list = new_all_test_list\n",
    "\n",
    "    # Split bounding box with train, val, test sets\n",
    "    imgs_format = 'jpg'\n",
    "    file = open(wwfood100_path + '/labels.txt', 'w')\n",
    "    for c in category_names:\n",
    "        file.write(c + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    #### Training set\n",
    "    file = open(wwfood100_path + '/' + files_generated[0] + '.txt', 'w')\n",
    "    file_1_shot = open(wwfood100_path + '/' + files_generated[0] + '_1_shot' + '.txt', 'w')\n",
    "    file_3_shot = open(wwfood100_path + '/' + files_generated[0] + '_3_shot' + '.txt', 'w')\n",
    "    file_5_shot = open(wwfood100_path + '/' + files_generated[0] + '_5_shot' + '.txt', 'w')\n",
    "    file_10_shot = open(wwfood100_path + '/' + files_generated[0] + '_10_shot' + '.txt', 'w')\n",
    "    #file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "    for img in all_train_list:\n",
    "        # it is possible that one image in several categories\n",
    "        occurrences = []\n",
    "        for id_index, id in enumerate(category_ids):\n",
    "            occ = [[wwfood100_path + '/Images/' + img + '.' + imgs_format, str(id)] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "            occurrences += occ\n",
    "\n",
    "        for idx, occ in enumerate(occurrences):\n",
    "            img_path = occ[0]\n",
    "            img_id = img_path.split('/')[-1].split('.')[0]\n",
    "            file.write(img_id + '\\n')\n",
    "            if idx<1:\n",
    "                file_1_shot.write(img_id + '\\n')\n",
    "            if idx<3:\n",
    "                file_3_shot.write(img_id + '\\n')\n",
    "            if idx<5:\n",
    "                file_5_shot.write(img_id + '\\n')\n",
    "            if idx<10:\n",
    "                file_10_shot.write(img_id + '\\n')\n",
    "            #img_category = occ[1]\n",
    "            #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "            #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "            \n",
    "    file.close()\n",
    "    file_1_shot.close()\n",
    "    file_3_shot.close()\n",
    "    file_5_shot.close()\n",
    "    file_10_shot.close()\n",
    "    \n",
    "    #### Val set\n",
    "    file = open(wwfood100_path + '/' + files_generated[1] + '.txt', 'w')\n",
    "    #file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "    for img in all_val_list:\n",
    "        # it is possible that one image in several categories\n",
    "        occurrences = []\n",
    "        for id_index, id in enumerate(category_ids):\n",
    "            occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "            occurrences += occ\n",
    "\n",
    "        for occ in occurrences:\n",
    "            img_path = occ[0]\n",
    "            img_id = img_path.split('/')[-1].split('.')[0]\n",
    "            file.write(img_id + '\\n')\n",
    "            #img_category = occ[1]\n",
    "            #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "            #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    #### Testing set\n",
    "    file = open(wwfood100_path + '/' + files_generated[2] + '.txt', 'w')\n",
    "    #file.write('img category_id x1 y1 x2 y2\\n')  # header\n",
    "    for img in all_test_list:\n",
    "        # it is possible that one image in several categories\n",
    "        occurrences = []\n",
    "        for id_index, id in enumerate(category_ids):\n",
    "            occ = [[uecfood100_path + '/' + str(id) + '/' + img + '.' + imgs_format, str(id)] for i, elem in enumerate(category_images[id_index]) if elem == img]\n",
    "            occurrences += occ\n",
    "\n",
    "        for occ in occurrences:\n",
    "            img_path = occ[0]\n",
    "            img_id = img_path.split('/')[-1].split('.')[0]\n",
    "            file.write(img_id + '\\n')\n",
    "            #img_category = occ[1]\n",
    "            #img_bbox = str(occ[2]) + ' ' + str(occ[3]) + ' ' + str(occ[4]) + ' ' + str(occ[5])\n",
    "            #file.write(img_path + ' ' + img_category + ' ' + img_bbox + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [0.7, 0.2, 0.1]\n",
    "files_generated = ['base_train', 'base_val', 'base_test']\n",
    "save_txt(files_generated, split, np.arange(len(base_original_category_idxs)), screened_base_category_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [0.1, 0.2, 0.7]\n",
    "files_generated = ['novel_train', 'novel_val', 'novel_test']\n",
    "save_txt(files_generated, split, np.arange(len(novel_original_category_idxs)), novel_category_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
